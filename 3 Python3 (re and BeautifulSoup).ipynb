{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intelligent-cooper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/icefox/.local/lib/python3.8/site-packages (4.9.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /home/icefox/.local/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "express-family",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /home/icefox/.local/lib/python3.8/site-packages (4.6.3)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiovascular-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Представим, что перед нами стоит задача проанализировать, какие слова необходимо учить человеку,\n",
    "#недостаточно владеющему техническим английским, для успешной работы в области Data Science\n",
    "#То есть нам нужно получить слова, наиболее часто встречающиеся в основной технической документации DS\n",
    "\n",
    "#Для решения задачи мы спарсим весь текст с сайтов документации sympy, pandas, skilearn\n",
    "#используя рекурсивный обход страниц сайта \"в глубину\" (как графа)\n",
    "#текст разберём на слова, приведём их к нормальной форме и составим статистики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-wales",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fundamental-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "frozen-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Общие настройки парсинга (глубина парсинга и User-agent)\n",
    "depth = 3\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:45.0) Gecko/20100101 Firefox/45.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bored-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Функции ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "insured-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_directory(page):\n",
    "    '''Фунция, которая по строке адреса страницы даёт строку адреса её директории'''\n",
    "    return page.url.rpartition('/')[0] + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "peripheral-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_all_text_and_links_from_page(link):\n",
    "    '''Функция выбора с текущей веб-страницы всего текста и всех ссылок, ведущих не более чем на depth уровней выше'''\n",
    "    page = requests.get(link, headers = headers)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    #Получим весь текст\n",
    "    p_tags = soup.find_all('p')\n",
    "    text =  [p_tag.text for p_tag in p_tags]\n",
    "    #И все ссылки\n",
    "    links = soup.find_all('a')\n",
    "    links = [link.get('href').rpartition('.html')[0] + '.html' for link in links]\n",
    "    #Удалим все дубликаты и ссылки, начинающиеся с точки\n",
    "    links = list(set(links))\n",
    "    links = list(filter( lambda x : not x.startswith(\".\"), links ))\n",
    "    #Возможно для типа ссылок: с абсолютными путями и оносительными. Приведём все ссылки к абсолютному виду\n",
    "    page_directory_link = page_directory(page)\n",
    "    links = [page_directory_link + current_link if not current_link.startswith(\"http\") else current_link for current_link in links]\n",
    "    #Ну и, наконец, оставим только ссылки, ведущие не более чем на depth уровней выше от исходной\n",
    "    links = list(filter( lambda x : x.startswith(page_directory_link), links ))\n",
    "    links = list(filter( lambda x : (x.count('/') - int_basic_level) < depth, links ))\n",
    "    return text, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "declared-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_recursion(link, all_text, all_links):\n",
    "    '''Функция рекурсивного обхода страниц сайта как графа в глубину, в которую как начальную вершину мы передадим базовую страницу'''\n",
    "    #В множество посещённых страничек добавим ту, что посещаем в данный момент\n",
    "    all_links.append(link)\n",
    "    #В весь текст с текущего домена добавим текст полученный с текущей ссылки\n",
    "    new_text, new_links = take_all_text_and_links_from_page(link)\n",
    "    all_text += new_text\n",
    "    #А к тем ссылкам с текущей страницы, которые ещё не были посещены, рекурсивно применим эту же функцию\n",
    "    new_links = list(set(new_links) - set(all_links))\n",
    "    for new_link in new_links:\n",
    "        graph_recursion(new_link, all_text, all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-coast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "referenced-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Сайты c документацией по основным библиотекам Data Science ###\n",
    "doc_urls = ['https://docs.sympy.org/latest/index.html',\n",
    "            'https://pandas.pydata.org/docs/user_guide/index.html',\n",
    "            'https://scikit-learn.org/dev/user_guide.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "available-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для сокращения времени выполнения проверим работоспособность на сайте scikit-learn\n",
    "#Далее при потребность уже легко обернуть в цикл по всему массиву ссылок\n",
    "basic_link = doc_urls[2]\n",
    "\n",
    "##Начальный уровень парсинга\n",
    "int_basic_level = basic_link.count('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "hairy-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Непосредственно сам процесс парсинга (получение всего содержимого сайта со всех страниц)\n",
    "all_text_from_domen = []\n",
    "all_links_from_domen = []\n",
    "graph_recursion(basic_link, all_text_from_domen, all_links_from_domen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "transsexual-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Обработка полученного текста ###\n",
    "import re\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "numeric-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_string = []\n",
    "\n",
    "for raw_text in all_text_from_domen:\n",
    "    # формируем датасет из отдельных слов, приводим к нижнему регистру\n",
    "    raw_text_lower = raw_text.lower()\n",
    "    # разбиваем текст на слова\n",
    "    regular_expr = r'\\w+'\n",
    "    reg_expr_compiled = re.compile(regular_expr)\n",
    "    text_by_words = reg_expr_compiled.findall(raw_text_lower) \n",
    "    text_by_words = list(filter(lambda x: x.isalpha(), text_by_words))\n",
    "    #Cразу при добавлении приводим слова к \"стандартной\" форме (\"счастливы\" -> \"счастье\")\n",
    "    for word in text_by_words:\n",
    "        words_in_string.append(morph.parse(word)[0].normal_form)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "decimal-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Собираем статистики по словам\n",
    "import pandas as pd\n",
    "\n",
    "tokens_df = pd.DataFrame({'word': words_in_string})\n",
    "# дамми-столбец\n",
    "tokens_df = tokens_df.assign(dummy = 1)\n",
    "# аггрегируем статистики\n",
    "word_count_df = tokens_df.groupby(['word'])['dummy'].count().reset_index().sort_values(by='dummy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "funky-tackle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11355</th>\n",
       "      <td>the</td>\n",
       "      <td>108833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7874</th>\n",
       "      <td>of</td>\n",
       "      <td>49870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11504</th>\n",
       "      <td>to</td>\n",
       "      <td>38485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>32107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>is</td>\n",
       "      <td>30707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>and</td>\n",
       "      <td>27269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>in</td>\n",
       "      <td>26432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>for</td>\n",
       "      <td>24904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>be</td>\n",
       "      <td>14236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>if</td>\n",
       "      <td>13371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word   count\n",
       "11355  the  108833\n",
       "7874    of   49870\n",
       "11504   to   38485\n",
       "0        a   32107\n",
       "5561    is   30707\n",
       "441    and   27269\n",
       "5204    in   26432\n",
       "4109   for   24904\n",
       "969     be   14236\n",
       "5103    if   13371"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Выведем первые 10 строк\n",
    "word_count_df.rename(columns = {'dummy' : 'count'}, inplace=True)\n",
    "word_count_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "warming-maryland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11404</th>\n",
       "      <td>this</td>\n",
       "      <td>12860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>with</td>\n",
       "      <td>11677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11354</th>\n",
       "      <td>that</td>\n",
       "      <td>11241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>data</td>\n",
       "      <td>10600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3581</th>\n",
       "      <td>estimator</td>\n",
       "      <td>8205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7801</th>\n",
       "      <td>number</td>\n",
       "      <td>7632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>each</td>\n",
       "      <td>6886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12428</th>\n",
       "      <td>will</td>\n",
       "      <td>6691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>samples</td>\n",
       "      <td>6497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216</th>\n",
       "      <td>parameters</td>\n",
       "      <td>6142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  count\n",
       "11404        this  12860\n",
       "12462        with  11677\n",
       "11354        that  11241\n",
       "2557         data  10600\n",
       "3581    estimator   8205\n",
       "7801       number   7632\n",
       "3266         each   6886\n",
       "12428        will   6691\n",
       "9942      samples   6497\n",
       "8216   parameters   6142"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ожидаемо, получили союзы. Отфильтруем датафрейм по длине слова\n",
    "word_count_df = word_count_df[word_count_df['word'].str.len() > 3]\n",
    "word_count_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bearing-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Уже лучше. Сохраним результаты в файл, чтобы в следующий раз уже просто считать его, выполнив ячейку ниже:\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "suspected-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('english_words.csv'):\n",
    "    #Если файл уже существует - считываем его\n",
    "    word_count_df  = pd.read_csv('english_words.csv', index_col='num')\n",
    "else:\n",
    "    #При первом же запуске нам нужно создать соответствующий CSV файл\n",
    "    word_count_df.index.names = ['num']\n",
    "    word_count_df.to_csv('english_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "greek-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получили желаемое: сортированный по частоте список слов в нормальной форме, встречающихся на всех страницах сайта\n",
    "#и количество их вхождений. \n",
    "\n",
    "#В дальнейшем данный файл будет расширен получением столбца с переводом слов и \n",
    "#автоматизацией логики изучения по алгоритму интервального повторения, с сохранением и визуализацией прогресса"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
